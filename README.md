# Fleet Maintenance Management (FMM) with LLM Advisor

This project is a **fleet maintenance management system** enhanced with **AI-powered advisory capabilities**.  
It integrates **LangGraph**, **OpenAI ChatGPT**, and AWS services to provide real-time explanations, actions, and urgency ratings based on telemetry and diagnostic data.

## ðŸš€ Features
- **LLM Advisor Node** (`app/agent/nodes_llm.py`)
  - Uses **OpenAI API**, with parameters configurable via `.env`
  - Structured JSON output with **Pydantic** (safe parsing)
  - Context-aware prompt including:
    - Telemetry data
    - Diagnostic Trouble Codes (DTC)
    - Failure probability
    - Maintenance flags
  - Business policy overlay to **raise or demote urgency** based on thresholds
  - Token usage tracking for OpenAI
- **AWS S3 Hosting**
  - Frontend assets served via S3 Static Website Hosting
  - API backend on EC2
- **Observability**
  - Tracks LLM token usage via `TokenUsageHandler`

## Installation

See [[src/README.MD]]

## ðŸ“‚ Project Structure
```

app/
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ nodes\_llm.py         # LLM advisor node
â”‚   â””â”€â”€ nodes\_ml.py          # ML prediction node
â”‚   â””â”€â”€ graph.py              # LangGraph Infrastructure
â”‚   â””â”€â”€ states.py             # state Class
â”œâ”€â”€ api/
â”‚   â””â”€â”€ main.py               # everything needed for FastAPI
â”œâ”€â”€ ml/
â”‚   â””â”€â”€ config.py             # config for ML Prediction
â”‚   â””â”€â”€ data_prep.py
â”‚   â””â”€â”€ features.py           # variable for features
â”‚   â””â”€â”€ lstm_arch.py
â”‚   â””â”€â”€ model_io.py           # streamlining model loading and scaling
â”‚   â””â”€â”€ best_model_08-18.pt   # saved model
â”‚   â””â”€â”€ scaler.save           # saved scaler
â”œâ”€â”€ observability/
â”‚   â””â”€â”€ instrumentation.py    
â”‚   â””â”€â”€ logging_setup.py
â”‚   â””â”€â”€ token_callback.py     # OpenAI GPT token usage handler
â”‚   â””â”€â”€ usage.py
â”œâ”€â”€ static/
â”‚   â””â”€â”€ index.html           # Frontend entry point (moved to S3 in prod)
â”œâ”€â”€ logs/                    # logging needs
```

## ðŸ§  How Urgency is Determined

1. **LLM Decision** â€“ The LLM outputs urgency based on context.
2. **Policy Overlay** â€“ Business rules **can raise or lower urgency** based on `failure_probability`.

   * `>= 0.7` â†’ **High**
   * `>= 0.3` â†’ **Medium**
   * `< 0.3` â†’ **Low**
3. If LLM and probability disagree:

   * Urgency is elevated for high risk, **user is notified through pop up message**.
   * Urgency is demoted if probability is low


## ðŸ“Š Token Usage Tracking

OpenAI calls are wrapped with `TokenUsageHandler` to log:

* Prompt tokens
* Completion tokens
* Total tokens
* Cost estimates

Which will be further saved into token.log.


